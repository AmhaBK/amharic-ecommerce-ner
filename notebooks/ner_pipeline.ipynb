{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c342c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mike\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "c:\\Users\\mike\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_19700\\3773994474.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=100, random_state=42) if len(x) >= 100 else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed auto-labeling complete. Output saved to: output/auto_labeled_fixed.conll\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "\n",
    "# Load NER model fine-tuned on multilingual data\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "ner_pipeline = pipeline(\"ner\", model=model_name, tokenizer=tokenizer, grouped_entities=False)\n",
    "\n",
    "# Load your scraped Telegram data\n",
    "df = pd.read_csv(\"../data/raw/telegram_data.csv\")\n",
    "\n",
    "# Sample up to 100 messages per channel\n",
    "sampled_df = (\n",
    "    df.dropna(subset=[\"Message\"])\n",
    "    .groupby(\"Channel Username\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=100, random_state=42) if len(x) >= 100 else x)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Function to clean Amharic text\n",
    "def clean_amharic_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = re.sub(r'[^\\u1200-\\u137F\\u1380-\\u139F\\u2D80-\\u2DDF0-9A-Za-z፡።፣፤፥፦፧.,!?()\\[\\]\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Final output lines in CoNLL format\n",
    "output_lines = []\n",
    "\n",
    "# Loop through each sampled message\n",
    "for _, row in sampled_df.iterrows():\n",
    "    raw_text = str(row[\"Message\"])\n",
    "    cleaned = clean_amharic_text(raw_text)\n",
    "    words = cleaned.split()\n",
    "\n",
    "    try:\n",
    "        ner_result = ner_pipeline(cleaned)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping message due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Build character-level label map\n",
    "    char_labels = [\"O\"] * len(cleaned)\n",
    "    for entity in ner_result:\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        label = entity['entity']\n",
    "        if label.startswith(\"B-\"):\n",
    "            char_labels[start] = label  # First character\n",
    "            for i in range(start + 1, end):\n",
    "                if i < len(char_labels) and char_labels[i] == \"O\":\n",
    "                    char_labels[i] = \"I-\" + label[2:]\n",
    "\n",
    "    # Now assign labels per word using majority char label\n",
    "    position = 0\n",
    "    output_lines.append(f\"# {row['Channel Username']} | ID: {row['Message ID']}\")\n",
    "    for word in words:\n",
    "        word_len = len(word)\n",
    "        # Get character labels for this word\n",
    "        word_char_labels = char_labels[position:position + word_len]\n",
    "        if not word_char_labels:\n",
    "            label = \"O\"\n",
    "        else:\n",
    "            # Majority label (excluding 'O' if possible)\n",
    "            non_o_labels = [l for l in word_char_labels if l != \"O\"]\n",
    "            label = non_o_labels[0] if non_o_labels else \"O\"\n",
    "        output_lines.append(f\"{word} {label}\")\n",
    "        position += word_len + 1  # Account for space\n",
    "    output_lines.append(\"\")\n",
    "\n",
    "# Save to file\n",
    "with open(\"output/auto_labeled_fixed.conll\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"✅ Fixed auto-labeling complete. Output saved to: output/auto_labeled_fixed.conll\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
